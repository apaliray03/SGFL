{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2219685a",
   "metadata": {
    "id": "2219685a"
   },
   "source": [
    "**MNIST DATASET | IID | CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b72bb6",
   "metadata": {
    "id": "98b72bb6"
   },
   "outputs": [],
   "source": [
    "!pip install -q flwr[\"simulation\"]\n",
    "import flwr as fl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# create clients and assign data to them\n",
    "def create_clients(image_list, label_list, num_clients, initial):\n",
    "\n",
    "    # create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i) for i in range(num_clients)]\n",
    "    # randomize the data\n",
    "    data         = list(zip(image_list, label_list))\n",
    "    np.random.shuffle(data)\n",
    "    # shard data and assign to each client\n",
    "    size         = len(data)//num_clients\n",
    "    shards       = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "    # number of clients must equal to number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n",
    "\n",
    "# batch assigned data of clients\n",
    "def batch_data(data_shard, batch_size):\n",
    "\n",
    "    # seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset     = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    dataset     = dataset.shuffle(len(label))\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# define initial CNN model for clients in Flower framework\n",
    "def client_fn(cid):\n",
    "\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(32,kernel_size=(3, 3),activation=\"relu\",input_shape=input_shape),\n",
    "            layers.MaxPool2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64,kernel_size=(3, 3),activation=\"relu\"),\n",
    "            layers.MaxPool2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(num_classes, activation=\"softmax\")\n",
    "        ]\n",
    "    )\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=learning_rate, decay=learning_rate / comms_round, beta_1=momentum), metrics=[\"accuracy\"])\n",
    "    return FlowerClient(model, clients[f\"client_{cid}\"])\n",
    "\n",
    "# configure FlowerClient parameters and functions\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "\n",
    "    # define each client's model and batched data\n",
    "    def __init__(self, model, data):\n",
    "\n",
    "        self.model = model\n",
    "        self.data  = batch_data(data, batch_size)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(self.data, epochs=1, verbose=0, validation_data=(x_test, y_test))\n",
    "\n",
    "        for(X_train, Y_train) in self.data:\n",
    "            return self.model.get_weights(), len(X_train), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, accuracy = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        return loss, len(x_test), {\"accuracy\": accuracy}\n",
    "\n",
    "# define save strategy for showing aggregated accuracy and aggregated loss at the end of each round\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "\n",
    "    def aggregate_evaluate(self,rnd,results,failures):\n",
    "\n",
    "      if not results:\n",
    "          return None, {}\n",
    "\n",
    "      aggregated_loss, aggregated_metrics = super().aggregate_evaluate(rnd, results, failures)\n",
    "      accuracies          = [r.metrics[\"accuracy\"] * r.num_examples for _, r in results]\n",
    "      examples            = [r.num_examples for _, r in results]\n",
    "      aggregated_accuracy = sum(accuracies) / sum(examples)\n",
    "      print(f\"comm_round: {rnd} | global_accuracy: {aggregated_accuracy} | global_loss: {aggregated_loss}\")\n",
    "      accuracy.append(aggregated_accuracy)\n",
    "      loss.append(aggregated_loss)\n",
    "      return aggregated_loss, {\"accuracy\": aggregated_accuracy}\n",
    "\n",
    "# define initial parameters\n",
    "num_classes   = 10\n",
    "num_clients   = 10\n",
    "input_shape   = (28, 28, 1)\n",
    "batch_size    = 128\n",
    "learning_rate = 0.01\n",
    "momentum      = 0.9\n",
    "comms_round   = 10\n",
    "accuracy      = list();\n",
    "loss          = list();\n",
    "\n",
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# scale dataset from [0,255] to [0,1]\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test  = x_test.astype(\"float32\") / 255\n",
    "# expand to 3d, add channels\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test  = np.expand_dims(x_test, -1)\n",
    "# change labels to categorical format\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# create clients for simulation\n",
    "clients = create_clients(x_train, y_train, num_clients, 'client')\n",
    "\n",
    "# start Flower simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=num_clients,\n",
    "    client_resources={\"num_cpus\": 2},\n",
    "    config=fl.server.ServerConfig(num_rounds=comms_round),\n",
    "    strategy=SaveModelStrategy()\n",
    ")\n",
    "\n",
    "# plot accuracy and loss of each round\n",
    "plt.suptitle('Accuracy Comparison')\n",
    "plt.scatter(range(1, comms_round+1), accuracy, c=[\"red\"])\n",
    "plt.show()\n",
    "plt.suptitle('Loss Comparison')\n",
    "plt.scatter(range(1, comms_round+1), loss, c=[\"red\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc2e6ac",
   "metadata": {
    "id": "9bc2e6ac"
   },
   "source": [
    "**MNIST DATASET | NON-IID | CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b0014",
   "metadata": {
    "id": "262b0014"
   },
   "outputs": [],
   "source": [
    "!pip install -q flwr[\"simulation\"]\n",
    "import flwr as fl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# create clients and assign data to them\n",
    "def create_clients(image_list, label_list, num_clients, initial, start):\n",
    "\n",
    "    # create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i) for i in range(start, start+num_clients)]\n",
    "    # randomize the data\n",
    "    data         = list(zip(image_list, label_list))\n",
    "    np.random.shuffle(data)\n",
    "    # shard data and assign to each client\n",
    "    size         = len(data)//num_clients\n",
    "    shards       = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "    # number of clients must equal to number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n",
    "\n",
    "# make data of each client non-iid\n",
    "def non_iid_x(image_list, label_list, severity, num_intraclass_clients):\n",
    "\n",
    "    non_iid_x_clients = dict()\n",
    "    # create unique label list and shuffle\n",
    "    unique_labels     = np.unique(label_list, axis=0)\n",
    "    np.random.shuffle(unique_labels)\n",
    "    # create sub label lists based on x\n",
    "    sub_lab_list      = [unique_labels[i:i + severity] for i in range(0, len(unique_labels), severity)]\n",
    "    count             = 0\n",
    "\n",
    "    for item in sub_lab_list:\n",
    "\n",
    "        # get all images for this label\n",
    "        class_data         = [(image, label) for (image, label) in list(zip(image_list, label_list)) if (item == label).all()]\n",
    "        # decouple tuple list into seperate image and label lists\n",
    "        images, labels     = zip(*class_data)\n",
    "        # create num_intraclass_clients clients from the class\n",
    "        intraclass_clients = create_clients(list(images), list(labels), num_intraclass_clients, 'client', count)\n",
    "        # append intraclass clients to main clients'dict\n",
    "        non_iid_x_clients.update(intraclass_clients)\n",
    "        count += num_intraclass_clients\n",
    "\n",
    "    return non_iid_x_clients\n",
    "\n",
    "# batch assigned data of clients\n",
    "def batch_data(data_shard, batch_size):\n",
    "\n",
    "    # seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset     = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    dataset     = dataset.shuffle(len(label))\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# define initial CNN model for clients in Flower framework\n",
    "def client_fn(cid):\n",
    "\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(32,kernel_size=(3, 3),activation=\"relu\",input_shape=input_shape),\n",
    "            layers.MaxPool2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64,kernel_size=(3, 3),activation=\"relu\"),\n",
    "            layers.MaxPool2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(num_classes, activation=\"softmax\")\n",
    "        ]\n",
    "    )\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=learning_rate, decay=learning_rate / comms_round, beta_1=momentum), metrics=[\"accuracy\"])\n",
    "    return FlowerClient(model, clients[f\"client_{cid}\"])\n",
    "\n",
    "# configure FlowerClient parameters and functions\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "\n",
    "    # define each client's model and batched data\n",
    "    def __init__(self, model, data):\n",
    "\n",
    "        self.model = model\n",
    "        self.data  = batch_data(data, batch_size)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(self.data, epochs=1, verbose=0, validation_data=(x_test, y_test))\n",
    "\n",
    "        for(X_train, Y_train) in self.data:\n",
    "            return self.model.get_weights(), len(X_train), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, accuracy = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        return loss, len(x_test), {\"accuracy\": accuracy}\n",
    "\n",
    "# define save strategy for showing aggregated accuracy and aggregated loss at the end of each round\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "\n",
    "    def aggregate_evaluate(self,rnd,results,failures):\n",
    "\n",
    "      if not results:\n",
    "          return None, {}\n",
    "\n",
    "      aggregated_loss, aggregated_metrics = super().aggregate_evaluate(rnd, results, failures)\n",
    "      accuracies          = [r.metrics[\"accuracy\"] * r.num_examples for _, r in results]\n",
    "      examples            = [r.num_examples for _, r in results]\n",
    "      aggregated_accuracy = sum(accuracies) / sum(examples)\n",
    "      print(f\"comm_round: {rnd} | global_accuracy: {aggregated_accuracy} | global_loss: {aggregated_loss}\")\n",
    "      accuracy.append(aggregated_accuracy)\n",
    "      loss.append(aggregated_loss)\n",
    "      return aggregated_loss, {\"accuracy\": aggregated_accuracy}\n",
    "\n",
    "# define initial parameters\n",
    "num_classes            = 10\n",
    "# number of sub clients to be created from each non-iid class\n",
    "num_intraclass_clients = 1\n",
    "num_clients            = num_classes * num_intraclass_clients\n",
    "input_shape            = (28, 28, 1)\n",
    "batch_size             = 128\n",
    "# non-iid severity, 1 means each client will only have one class of data\n",
    "severity               = 1\n",
    "learning_rate          = 0.01\n",
    "momentum               = 0.9\n",
    "comms_round            = 10\n",
    "accuracy               = list();\n",
    "loss                   = list();\n",
    "\n",
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# scale dataset from [0,255] to [0,1]\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test  = x_test.astype(\"float32\") / 255\n",
    "# expand to 3d, add channels\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test  = np.expand_dims(x_test, -1)\n",
    "# change labels to categorical format\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# create non-iid clients for simulation\n",
    "clients = non_iid_x(x_train, y_train, severity, num_intraclass_clients)\n",
    "\n",
    "# start Flower simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=num_clients,\n",
    "    client_resources={\"num_cpus\": 2},\n",
    "    config=fl.server.ServerConfig(num_rounds=comms_round),\n",
    "    strategy=SaveModelStrategy()\n",
    ")\n",
    "\n",
    "# plot accuracy and loss of each round\n",
    "plt.suptitle('Accuracy Comparison')\n",
    "plt.scatter(range(1, comms_round+1), accuracy, c=[\"red\"])\n",
    "plt.show()\n",
    "plt.suptitle('Loss Comparison')\n",
    "plt.scatter(range(1, comms_round+1), loss, c=[\"red\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba546a9",
   "metadata": {
    "id": "3ba546a9"
   },
   "source": [
    "**MNIST DATASET | NON-IID | SGFL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc5586",
   "metadata": {
    "id": "a6fc5586",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q flwr[\"simulation\"]\n",
    "import flwr as fl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# create clients and assign data to them\n",
    "def create_clients(image_list, label_list, num_clients, initial, start):\n",
    "\n",
    "    # create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i) for i in range(start, start+num_clients)]\n",
    "    # randomize the data\n",
    "    data         = list(zip(image_list, label_list))\n",
    "    np.random.shuffle(data)\n",
    "    # shard data and assign to each client\n",
    "    size         = len(data)//num_clients\n",
    "    shards       = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "    # number of clients must equal to number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n",
    "\n",
    "# make data of each client non-iid\n",
    "def non_iid_x(image_list, label_list, severity, num_intraclass_clients):\n",
    "\n",
    "    non_iid_x_clients = dict()\n",
    "    #create unique label list and shuffle\n",
    "    unique_labels     = np.unique(label_list, axis=0)\n",
    "    np.random.shuffle(unique_labels)\n",
    "    # create sub label lists based on x\n",
    "    sub_lab_list      = [unique_labels[i:i + severity] for i in range(0, len(unique_labels), severity)]\n",
    "    count             = 0\n",
    "\n",
    "    for item in sub_lab_list:\n",
    "\n",
    "        # get all images for this label\n",
    "        class_data         = [(image, label) for (image, label) in list(zip(image_list, label_list)) if (item == label).all()]\n",
    "        # decouple tuple list into seperate image and label lists\n",
    "        images, labels     = zip(*class_data)\n",
    "        # create num_intraclass_clients clients from the class\n",
    "        intraclass_clients = create_clients(list(images), list(labels), num_intraclass_clients, 'client', count)\n",
    "        # append intraclass clients to main clients'dict\n",
    "        non_iid_x_clients.update(intraclass_clients)\n",
    "        count += num_intraclass_clients\n",
    "\n",
    "    return non_iid_x_clients\n",
    "\n",
    "# batch assigned data of clients\n",
    "def batch_data(data_shard, batch_size):\n",
    "\n",
    "    # seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset     = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    dataset     = dataset.shuffle(len(label))\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# define initial SDCGAN model for clients in Flower framework\n",
    "def client_fn(cid):\n",
    "\n",
    "    model           = load_model('mnist_5990.h5')\n",
    "    model.trainable = True\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "    return FlowerClient(model, clients[f\"client_{cid}\"])\n",
    "\n",
    "# configure FlowerClient parameters and functions\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "\n",
    "    # define each client's model and batched data\n",
    "    def __init__(self, model, data):\n",
    "\n",
    "        self.model = model\n",
    "        self.data  = batch_data(data, batch_size)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(self.data, epochs=1, verbose=0, validation_data=(x_test, y_test))\n",
    "\n",
    "        for(X_train, Y_train) in self.data:\n",
    "            return self.model.get_weights(), len(X_train), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, accuracy = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        return loss, len(x_test), {\"accuracy\": accuracy}\n",
    "\n",
    "# define save strategy for showing aggregated accuracy and aggregated loss at the end of each round\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "\n",
    "    def aggregate_evaluate(self,rnd,results,failures):\n",
    "\n",
    "      if not results:\n",
    "          return None, {}\n",
    "\n",
    "      aggregated_loss, aggregated_metrics = super().aggregate_evaluate(rnd, results, failures)\n",
    "      accuracies          = [r.metrics[\"accuracy\"] * r.num_examples for _, r in results]\n",
    "      examples            = [r.num_examples for _, r in results]\n",
    "      aggregated_accuracy = sum(accuracies) / sum(examples)\n",
    "      print(f\"comm_round: {rnd} | global_accuracy: {aggregated_accuracy} | global_loss: {aggregated_loss}\")\n",
    "      accuracy.append(aggregated_accuracy)\n",
    "      loss.append(aggregated_loss)\n",
    "      return aggregated_loss, {\"accuracy\": aggregated_accuracy}\n",
    "\n",
    "# custom activation function\n",
    "def custom_activation(output):\n",
    "\n",
    "    logexpsum = backend.sum(backend.exp(output), axis=-1, keepdims=True)\n",
    "    result    = logexpsum / (logexpsum + 1.0)\n",
    "    return result\n",
    "\n",
    "# define the standalone supervised and unsupervised discriminator models\n",
    "def define_discriminator(input_shape, num_classes):\n",
    "\n",
    "     # image input\n",
    "     input_image = layers.Input(shape=input_shape)\n",
    "     # downsample\n",
    "     fe          = layers.Conv2D(128, (3,3), strides=(2,2), padding='same')(input_image)\n",
    "     fe          = layers.LeakyReLU(alpha=0.2)(fe)\n",
    "     # downsample\n",
    "     fe          = layers.Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "     fe          = layers.LeakyReLU(alpha=0.2)(fe)\n",
    "     # downsample\n",
    "     fe          = layers.Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "     fe          = layers.LeakyReLU(alpha=0.2)(fe)\n",
    "     fe          = layers.Flatten()(fe)\n",
    "     fe          = layers.Dropout(0.4)(fe)\n",
    "     # output layer nodes\n",
    "     fe          = layers.Dense(num_classes)(fe)\n",
    "     # supervised output\n",
    "     c_out_layer = layers.Activation('softmax')(fe)\n",
    "     # define and compile supervised discriminator model\n",
    "     c_model     = Model(input_image, c_out_layer)\n",
    "     c_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "     # unsupervised output\n",
    "     d_out_layer = layers.Lambda(custom_activation)(fe)\n",
    "     # define and compile unsupervised discriminator model\n",
    "     d_model     = Model(input_image, d_out_layer)\n",
    "     d_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "     return d_model, c_model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "\n",
    "     # image generator input\n",
    "     input_latent = layers.Input(shape=(latent_dim,))\n",
    "     # foundation for 7x7 image\n",
    "     gen          = layers.Dense(128 * 7 * 7)(input_latent)\n",
    "     gen          = layers.LeakyReLU(alpha=0.2)(gen)\n",
    "     gen          = layers.Reshape((7, 7, 128))(gen)\n",
    "     # upsample to 14x14\n",
    "     gen          = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "     gen          = layers.LeakyReLU(alpha=0.2)(gen)\n",
    "     # upsample to 28x28\n",
    "     gen          = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "     gen          = layers.LeakyReLU(alpha=0.2)(gen)\n",
    "     # output\n",
    "     out_layer    = layers.Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
    "     # define model\n",
    "     model        = Model(input_latent, out_layer)\n",
    "     return model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "\n",
    "     # make weights in the discriminator not trainable\n",
    "     d_model.trainable = False\n",
    "     # connect image output from generator as input to discriminator\n",
    "     gan_output        = d_model(g_model.output)\n",
    "     # define gan model as taking noise and outputting a classification\n",
    "     model             = Model(g_model.input, gan_output)\n",
    "     # compile model\n",
    "     model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "     return model\n",
    "\n",
    "# select a supervised subset of the dataset, ensures classes are balanced\n",
    "def select_supervised_samples(dataset, num_samples, num_classes):\n",
    "\n",
    "    X, y           = dataset\n",
    "    unique_labels  = np.unique(y, axis=0)\n",
    "    X_list, y_list = list(), list()\n",
    "    n_per_class    = int(num_samples / num_classes)\n",
    "\n",
    "    for i in unique_labels:\n",
    "\n",
    "        # get all images for this class\n",
    "        class_data   = [(image, label) for (image, label) in list(zip(X, y)) if (i == label).all()]\n",
    "        X_with_class, _ = zip(*class_data)\n",
    "        # choose random instances\n",
    "        ix           = np.random.randint(0, len(X_with_class), n_per_class)\n",
    "        # add to list\n",
    "        [X_list.append(X_with_class[j]) for j in ix]\n",
    "        [y_list.append(i) for j in ix]\n",
    "\n",
    "    return np.asarray(X_list), np.asarray(y_list)\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, num_samples):\n",
    "\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix             = np.random.randint(0, images.shape[0], num_samples)\n",
    "    # select images and labels\n",
    "    X, labels      = images[ix], labels[ix]\n",
    "    # generate class labels\n",
    "    y              = np.ones((num_samples, 1))\n",
    "    return [X, labels], y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, num_samples):\n",
    "\n",
    "    # generate points in the latent space\n",
    "    z_input = np.random.randn(latent_dim * num_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = z_input.reshape(num_samples, latent_dim)\n",
    "    return z_input\n",
    "\n",
    "# use the generator to generate fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, num_samples):\n",
    "\n",
    "    # generate points in latent space\n",
    "    z_input = generate_latent_points(latent_dim, num_samples)\n",
    "    # predict outputs\n",
    "    images  = generator.predict(z_input)\n",
    "    # create class labels\n",
    "    y       = np.zeros((num_samples, 1))\n",
    "    return images, y\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, c_model, latent_dim, dataset, num_samples):\n",
    "\n",
    "    # prepare fake examples\n",
    "    X, _ = generate_fake_samples(g_model, latent_dim, num_samples)\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    X    = (X + 1) / 2.0\n",
    "\n",
    "    # plot images\n",
    "    for i in range(100):\n",
    "\n",
    "        plt.subplot(10, 10, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "\n",
    "    # save plot to file\n",
    "    #filename = 'generated_plot_%04d.png' % (step+1)\n",
    "    #plt.savefig(filename)\n",
    "    plt.show()\n",
    "    # save the classifier model\n",
    "    filename = 'mnist_%04d.h5' % (step+1)\n",
    "    c_model.save(filename)\n",
    "\n",
    "    # evaluate the classifier model\n",
    "    X, y   = dataset\n",
    "    _, acc = c_model.evaluate(X, y, verbose=0)\n",
    "    print('Classifier Accuracy: %.3f%%' % (acc * 100))\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, c_model, gan_model, dataset, latent_dim, num_samples, num_classes, num_epochs, num_batch):\n",
    "\n",
    "    # select supervised dataset\n",
    "    X_sup, y_sup = select_supervised_samples(dataset, num_samples, num_classes)\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo  = int(dataset[0].shape[0] / num_batch)\n",
    "     # calculate the number of training iterations\n",
    "    n_steps      = bat_per_epo * num_epochs\n",
    "    # calculate the size of half a batch of samples\n",
    "    half_batch   = int(num_batch / 2)\n",
    "    print('num_epochs=%d, num_batch=%d, 1/2=%d, b/e=%d, steps=%d' % (num_epochs, num_batch, half_batch, bat_per_epo, n_steps))\n",
    "\n",
    "    for i in range(n_steps):\n",
    "\n",
    "         # update supervised discriminator (c)\n",
    "        [Xsup_real, ysup_real], _ = generate_real_samples([X_sup, y_sup], half_batch)\n",
    "        c_loss, c_acc             = c_model.train_on_batch(Xsup_real, ysup_real)\n",
    "        # update unsupervised discriminator (d)\n",
    "        [X_real, _], y_real       = generate_real_samples(dataset, half_batch)\n",
    "        d_loss1                   = d_model.train_on_batch(X_real, y_real)\n",
    "        X_fake, y_fake            = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        d_loss2                   = d_model.train_on_batch(X_fake, y_fake)\n",
    "        # update generator (g)\n",
    "        X_gan, y_gan              = generate_latent_points(latent_dim, num_batch), np.ones((num_batch, 1))\n",
    "        g_loss                    = gan_model.train_on_batch(X_gan, y_gan)\n",
    "        # summarize loss on this batch\n",
    "        print('>%d, c[%.3f,%.0f], d[%.3f,%.3f], g[%.3f]' % (i+1, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n",
    "\n",
    "        # evaluate the model performance every so often\n",
    "        if (i+1) % bat_per_epo == 0:\n",
    "\n",
    "            summarize_performance(i, g_model, c_model, latent_dim, dataset, num_samples)\n",
    "\n",
    "# define initial parameters\n",
    "num_classes            = 10\n",
    "# number of sub clients to be created from each non-iid class\n",
    "num_intraclass_clients = 1\n",
    "num_clients            = num_classes * num_intraclass_clients\n",
    "input_shape            = (28, 28, 1)\n",
    "batch_size             = 128\n",
    "# non-iid severity, 1 means each client will only have one class of data\n",
    "severity               = 1\n",
    "comms_round            = 10\n",
    "clientDataPercent      = 5\n",
    "# size of the latent space\n",
    "latent_dim             = 128\n",
    "num_samples            = 100\n",
    "num_epochs             = 20\n",
    "num_batch              = 100\n",
    "accuracy               = list()\n",
    "loss                   = list()\n",
    "clientsData            = list()\n",
    "\n",
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# scale dataset from [0,255] to [-1,1]\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test  = x_test.astype(\"float32\")\n",
    "x_train = (x_train - 127.5) / 127.5\n",
    "x_test  = (x_test - 127.5) / 127.5\n",
    "# expand to 3d, add channels\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test  = np.expand_dims(x_test, -1)\n",
    "# change labels to categorical format\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# create non-iid clients for simulation\n",
    "clients = non_iid_x(x_train, y_train, severity, num_intraclass_clients)\n",
    "\n",
    "# collect a percent of clients data\n",
    "for (client_name, data) in clients.items():\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "    data_temp                = data[:int(len(data) * clientDataPercent / 100)]\n",
    "    x_client, y_client       = list(zip(*data_temp))\n",
    "    x_client                 = np.asarray(x_client)\n",
    "    y_client                 = np.asarray(y_client)\n",
    "    clientgen                = ImageDataGenerator(rotation_range=5, width_shift_range=0.02, height_shift_range=0.02, shear_range=0.1, zoom_range=0.1, fill_mode='nearest')\n",
    "    clientgen.fit(x_client)\n",
    "    g                        = clientgen.flow(x_client, y_client, batch_size=len(x_client), shuffle=True)\n",
    "    x_redundant, y_redundant = next(g)\n",
    "    clientsData.extend(list(zip(x_redundant, y_redundant)))\n",
    "    np.random.shuffle(clientsData)\n",
    "\n",
    "np.random.shuffle(clientsData)\n",
    "x_clients, y_clients = list(zip(*clientsData))\n",
    "x_clients            = np.asarray(x_clients)\n",
    "y_clients            = np.asarray(y_clients)\n",
    "\n",
    "# augment the collected data from clients\n",
    "datagen              = ImageDataGenerator(rotation_range=10, width_shift_range=0.05, height_shift_range=0.05, shear_range=0.2, zoom_range=0.2, fill_mode='nearest')\n",
    "datagen.fit(x_clients)\n",
    "g                    = datagen.flow(x_clients, y_clients, batch_size=len(x_clients), shuffle=True)\n",
    "# create a new dataset based on augmented data\n",
    "x_created, y_created = next(g)\n",
    "last_step            = int(((len(x_clients) * 100) / clientDataPercent) / len(x_clients)) - 1\n",
    "current_step         = 1\n",
    "\n",
    "while current_step <= last_step:\n",
    "\n",
    "    x_temp, y_temp = next(g)\n",
    "    x_created = np.concatenate((x_created, x_temp))\n",
    "    y_created = np.concatenate((y_created, y_temp))\n",
    "    current_step += 1\n",
    "\n",
    "# create the discriminator models\n",
    "d_model, c_model = define_discriminator(input_shape, num_classes)\n",
    "# create the generator\n",
    "g_model          = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model        = define_gan(g_model, d_model)\n",
    "# train model with the created dataset\n",
    "train(g_model, d_model, c_model, gan_model, [x_created, y_created], latent_dim, num_samples, num_classes, num_epochs, num_batch)\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = c_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train Accuracy: %.3f%%' % (train_acc * 100))\n",
    "_, test_acc  = c_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Accuracy: %.3f%%' % (test_acc * 100))\n",
    "\n",
    "# start Flower simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=num_clients,\n",
    "    client_resources={\"num_cpus\": 2},\n",
    "    config=fl.server.ServerConfig(num_rounds=comms_round),\n",
    "    strategy=SaveModelStrategy()\n",
    ")\n",
    "\n",
    "# plot accuracy and loss of each round\n",
    "plt.suptitle('Accuracy Comparison')\n",
    "plt.scatter(range(1, comms_round+1), accuracy, c=[\"red\"])\n",
    "plt.show()\n",
    "plt.suptitle('Loss Comparison')\n",
    "plt.scatter(range(1, comms_round+1), loss, c=[\"red\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b011365",
   "metadata": {
    "id": "9b011365"
   },
   "source": [
    "**EMNIST DATASET | IID | CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c5814b",
   "metadata": {
    "id": "c0c5814b"
   },
   "outputs": [],
   "source": [
    "!pip install -q flwr[\"simulation\"]\n",
    "import flwr as fl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# create clients and assign data to them\n",
    "def create_clients(image_list, label_list, num_clients, initial):\n",
    "\n",
    "    # create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i) for i in range(num_clients)]\n",
    "    # randomize the data\n",
    "    data         = list(zip(image_list, label_list))\n",
    "    np.random.shuffle(data)\n",
    "    # shard data and assign to each client\n",
    "    size         = len(data)//num_clients\n",
    "    shards       = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "    # number of clients must equal to number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n",
    "\n",
    "# batch assigned data of clients\n",
    "def batch_data(data_shard, batch_size):\n",
    "\n",
    "    # seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset     = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    dataset     = dataset.shuffle(len(label))\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# define initial CNN model for clients in Flower framework\n",
    "def client_fn(cid):\n",
    "\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(32,kernel_size=(3, 3),activation=\"relu\",input_shape=input_shape),\n",
    "            layers.MaxPool2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64,kernel_size=(3, 3),activation=\"relu\"),\n",
    "            layers.MaxPool2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(num_classes, activation=\"softmax\")\n",
    "        ]\n",
    "    )\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=learning_rate, decay=learning_rate / comms_round, beta_1=momentum), metrics=[\"accuracy\"])\n",
    "    return FlowerClient(model, clients[f\"client_{cid}\"])\n",
    "\n",
    "# configure FlowerClient parameters and functions\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "\n",
    "    # define each client's model and batched data\n",
    "    def __init__(self, model, data):\n",
    "\n",
    "        self.model = model\n",
    "        self.data  = batch_data(data, batch_size)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(self.data, epochs=1, verbose=0, validation_data=(x_test, y_test))\n",
    "\n",
    "        for(X_train, Y_train) in self.data:\n",
    "            return self.model.get_weights(), len(X_train), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, accuracy = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        return loss, len(x_test), {\"accuracy\": accuracy}\n",
    "\n",
    "# define save strategy for showing aggregated accuracy and aggregated loss at the end of each round\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "\n",
    "    def aggregate_evaluate(self,rnd,results,failures):\n",
    "\n",
    "      if not results:\n",
    "          return None, {}\n",
    "\n",
    "      aggregated_loss, aggregated_metrics = super().aggregate_evaluate(rnd, results, failures)\n",
    "      accuracies          = [r.metrics[\"accuracy\"] * r.num_examples for _, r in results]\n",
    "      examples            = [r.num_examples for _, r in results]\n",
    "      aggregated_accuracy = sum(accuracies) / sum(examples)\n",
    "      print(f\"comm_round: {rnd} | global_accuracy: {aggregated_accuracy} | global_loss: {aggregated_loss}\")\n",
    "      accuracy.append(aggregated_accuracy)\n",
    "      loss.append(aggregated_loss)\n",
    "      return aggregated_loss, {\"accuracy\": aggregated_accuracy}\n",
    "\n",
    "# define initial parameters\n",
    "num_classes   = 47\n",
    "num_clients   = 47\n",
    "input_shape   = (28, 28, 1)\n",
    "batch_size    = 128\n",
    "learning_rate = 0.01\n",
    "momentum      = 0.9\n",
    "comms_round   = 10\n",
    "accuracy      = list();\n",
    "loss          = list();\n",
    "\n",
    "# read dataset from csv files and reshape it to the input_shape\n",
    "dateset_train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
    "dateset_train = dateset_train.to_numpy()\n",
    "x_train_temp  = dateset_train[:,1:785]\n",
    "\n",
    "x_train       = np.zeros(shape=(112800, 28, 28), dtype=np.uint8)\n",
    "for idx, n in enumerate(x_train_temp):\n",
    "    x_train[idx] = np.reshape(n, (28, 28))\n",
    "\n",
    "y_train       = dateset_train[:,0]\n",
    "\n",
    "dateset_test  = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
    "dateset_test  = dateset_test.to_numpy()\n",
    "x_test_temp   = dateset_test[:,1:785]\n",
    "\n",
    "x_test        = np.zeros(shape=(18800, 28, 28), dtype=np.uint8)\n",
    "for idx, n in enumerate(x_test_temp):\n",
    "    x_test[idx] = np.reshape(n, (28, 28))\n",
    "\n",
    "y_test        = dateset_test[:,0]\n",
    "\n",
    "# scale dataset from [0,255] to [0,1]\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test  = x_test.astype(\"float32\") / 255\n",
    "# expand to 3d, add channels\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test  = np.expand_dims(x_test, -1)\n",
    "# change labels to categorical format\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# create clients for simulation\n",
    "clients = create_clients(x_train, y_train, num_clients, 'client')\n",
    "\n",
    "# start Flower simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=num_clients,\n",
    "    client_resources={\"num_cpus\": 2},\n",
    "    config=fl.server.ServerConfig(num_rounds=comms_round),\n",
    "    strategy=SaveModelStrategy()\n",
    ")\n",
    "\n",
    "# plot accuracy and loss of each round\n",
    "plt.suptitle('Accuracy Comparison')\n",
    "plt.scatter(range(1, comms_round+1), accuracy, c=[\"red\"])\n",
    "plt.show()\n",
    "plt.suptitle('Loss Comparison')\n",
    "plt.scatter(range(1, comms_round+1), loss, c=[\"red\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62781501",
   "metadata": {
    "id": "62781501"
   },
   "source": [
    "**EMNIST DATASET | NON-IID | CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe02122",
   "metadata": {
    "id": "fbe02122"
   },
   "outputs": [],
   "source": [
    "!pip install -q flwr[\"simulation\"]\n",
    "import flwr as fl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# create clients and assign data to them\n",
    "def create_clients(image_list, label_list, num_clients, initial, start):\n",
    "\n",
    "    # create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i) for i in range(start, start+num_clients)]\n",
    "    # randomize the data\n",
    "    data         = list(zip(image_list, label_list))\n",
    "    np.random.shuffle(data)\n",
    "    # shard data and assign to each client\n",
    "    size         = len(data)//num_clients\n",
    "    shards       = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "    # number of clients must equal to number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n",
    "\n",
    "# make data of each client non-iid\n",
    "def non_iid_x(image_list, label_list, severity, num_intraclass_clients):\n",
    "\n",
    "    non_iid_x_clients = dict()\n",
    "    #create unique label list and shuffle\n",
    "    unique_labels     = np.unique(label_list, axis=0)\n",
    "    np.random.shuffle(unique_labels)\n",
    "    # create sub label lists based on x\n",
    "    sub_lab_list      = [unique_labels[i:i + severity] for i in range(0, len(unique_labels), severity)]\n",
    "    count             = 0\n",
    "\n",
    "    for item in sub_lab_list:\n",
    "\n",
    "        # get all images for this label\n",
    "        class_data         = [(image, label) for (image, label) in list(zip(image_list, label_list)) if (item == label).all()]\n",
    "        # decouple tuple list into seperate image and label lists\n",
    "        images, labels     = zip(*class_data)\n",
    "        # create num_intraclass_clients clients from the class\n",
    "        intraclass_clients = create_clients(list(images), list(labels), num_intraclass_clients, 'client', count)\n",
    "        # append intraclass clients to main clients'dict\n",
    "        non_iid_x_clients.update(intraclass_clients)\n",
    "        count += num_intraclass_clients\n",
    "\n",
    "    return non_iid_x_clients\n",
    "\n",
    "# batch assigned data of clients\n",
    "def batch_data(data_shard, batch_size):\n",
    "\n",
    "    # seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset     = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    dataset     = dataset.shuffle(len(label))\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# define initial CNN model for clients in Flower framework\n",
    "def client_fn(cid):\n",
    "\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(32,kernel_size=(3, 3),activation=\"relu\",input_shape=input_shape),\n",
    "            layers.MaxPool2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64,kernel_size=(3, 3),activation=\"relu\"),\n",
    "            layers.MaxPool2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(num_classes, activation=\"softmax\")\n",
    "        ]\n",
    "    )\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=learning_rate, decay=learning_rate / comms_round, beta_1=momentum), metrics=[\"accuracy\"])\n",
    "    return FlowerClient(model, clients[f\"client_{cid}\"])\n",
    "\n",
    "# configure FlowerClient parameters and functions\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "\n",
    "    # define each client's model and batched data\n",
    "    def __init__(self, model, data):\n",
    "\n",
    "        self.model = model\n",
    "        self.data  = batch_data(data, batch_size)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(self.data, epochs=1, verbose=0, validation_data=(x_test, y_test))\n",
    "\n",
    "        for(X_train, Y_train) in self.data:\n",
    "            return self.model.get_weights(), len(X_train), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, accuracy = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        return loss, len(x_test), {\"accuracy\": accuracy}\n",
    "\n",
    "# define save strategy for showing aggregated accuracy and aggregated loss at the end of each round\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "\n",
    "    def aggregate_evaluate(self,rnd,results,failures):\n",
    "\n",
    "      if not results:\n",
    "          return None, {}\n",
    "\n",
    "      aggregated_loss, aggregated_metrics = super().aggregate_evaluate(rnd, results, failures)\n",
    "      accuracies          = [r.metrics[\"accuracy\"] * r.num_examples for _, r in results]\n",
    "      examples            = [r.num_examples for _, r in results]\n",
    "      aggregated_accuracy = sum(accuracies) / sum(examples)\n",
    "      print(f\"comm_round: {rnd} | global_accuracy: {aggregated_accuracy} | global_loss: {aggregated_loss}\")\n",
    "      accuracy.append(aggregated_accuracy)\n",
    "      loss.append(aggregated_loss)\n",
    "      return aggregated_loss, {\"accuracy\": aggregated_accuracy}\n",
    "\n",
    "# define initial parameters\n",
    "num_classes            = 47\n",
    "# number of sub clients to be created from each non-iid class\n",
    "num_intraclass_clients = 1\n",
    "num_clients            = num_classes * num_intraclass_clients\n",
    "input_shape            = (28, 28, 1)\n",
    "batch_size             = 128\n",
    "# non-iid severity, 1 means each client will only have one class of data\n",
    "severity               = 1\n",
    "learning_rate          = 0.001\n",
    "momentum               = 0.5\n",
    "comms_round            = 10\n",
    "accuracy               = list();\n",
    "loss                   = list();\n",
    "\n",
    "# read dataset from csv files and reshape it to the input_shape\n",
    "dateset_train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
    "dateset_train = dateset_train.to_numpy()\n",
    "x_train_temp  = dateset_train[:,1:785]\n",
    "\n",
    "x_train       = np.zeros(shape=(112800, 28, 28), dtype=np.uint8)\n",
    "for idx, n in enumerate(x_train_temp):\n",
    "    x_train[idx] = np.reshape(n, (28, 28))\n",
    "\n",
    "y_train       = dateset_train[:,0]\n",
    "\n",
    "dateset_test  = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
    "dateset_test  = dateset_test.to_numpy()\n",
    "x_test_temp   = dateset_test[:,1:785]\n",
    "\n",
    "x_test        = np.zeros(shape=(18800, 28, 28), dtype=np.uint8)\n",
    "for idx, n in enumerate(x_test_temp):\n",
    "    x_test[idx] = np.reshape(n, (28, 28))\n",
    "\n",
    "y_test        = dateset_test[:,0]\n",
    "\n",
    "# scale dataset from [0,255] to [0,1]\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test  = x_test.astype(\"float32\") / 255\n",
    "# expand to 3d, add channels\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test  = np.expand_dims(x_test, -1)\n",
    "# change labels to categorical format\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# create non-iid clients for simulation\n",
    "clients = non_iid_x(x_train, y_train, severity, num_intraclass_clients)\n",
    "\n",
    "# start Flower simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=num_clients,\n",
    "    client_resources={\"num_cpus\": 2},\n",
    "    config=fl.server.ServerConfig(num_rounds=comms_round),\n",
    "    strategy=SaveModelStrategy()\n",
    ")\n",
    "\n",
    "# plot accuracy and loss of each round\n",
    "plt.suptitle('Accuracy Comparison')\n",
    "plt.scatter(range(1, comms_round+1), accuracy, c=[\"red\"])\n",
    "plt.show()\n",
    "plt.suptitle('Loss Comparison')\n",
    "plt.scatter(range(1, comms_round+1), loss, c=[\"red\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d5ec5",
   "metadata": {
    "id": "558d5ec5"
   },
   "source": [
    "**EMNIST DATASET | NON-IID | SGFL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770e9e2",
   "metadata": {
    "id": "7770e9e2"
   },
   "outputs": [],
   "source": [
    "!pip install -q flwr[\"simulation\"]\n",
    "import flwr as fl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# create clients and assign data to them\n",
    "def create_clients(image_list, label_list, num_clients, initial, start):\n",
    "\n",
    "    # create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i) for i in range(start, start+num_clients)]\n",
    "    # randomize the data\n",
    "    data         = list(zip(image_list, label_list))\n",
    "    np.random.shuffle(data)\n",
    "    # shard data and assign to each client\n",
    "    size         = len(data)//num_clients\n",
    "    shards       = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "    # number of clients must equal to number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n",
    "\n",
    "# make data of each client non-iid\n",
    "def non_iid_x(image_list, label_list, severity, num_intraclass_clients):\n",
    "\n",
    "    non_iid_x_clients = dict()\n",
    "    #create unique label list and shuffle\n",
    "    unique_labels     = np.unique(label_list, axis=0)\n",
    "    np.random.shuffle(unique_labels)\n",
    "    # create sub label lists based on x\n",
    "    sub_lab_list      = [unique_labels[i:i + severity] for i in range(0, len(unique_labels), severity)]\n",
    "    count             = 0\n",
    "\n",
    "    for item in sub_lab_list:\n",
    "\n",
    "        # get all images for this label\n",
    "        class_data         = [(image, label) for (image, label) in list(zip(image_list, label_list)) if (item == label).all()]\n",
    "        # decouple tuple list into seperate image and label lists\n",
    "        images, labels     = zip(*class_data)\n",
    "        # create num_intraclass_clients clients from the class\n",
    "        intraclass_clients = create_clients(list(images), list(labels), num_intraclass_clients, 'client', count)\n",
    "        # append intraclass clients to main clients'dict\n",
    "        non_iid_x_clients.update(intraclass_clients)\n",
    "        count += num_intraclass_clients\n",
    "\n",
    "    return non_iid_x_clients\n",
    "\n",
    "# batch assigned data of clients\n",
    "def batch_data(data_shard, batch_size):\n",
    "\n",
    "    # seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset     = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    dataset     = dataset.shuffle(len(label))\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# define initial SDCGAN model for clients in Flower framework\n",
    "def client_fn(cid):\n",
    "\n",
    "    model           = load_model('emnist_16920.h5')\n",
    "    model.trainable = True\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001, beta_1=0.5), metrics=['accuracy'])\n",
    "    return FlowerClient(model, clients[f\"client_{cid}\"])\n",
    "\n",
    "# configure FlowerClient parameters and functions\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "\n",
    "    # define each client's model and batched data\n",
    "    def __init__(self, model, data):\n",
    "\n",
    "        self.model = model\n",
    "        self.data  = batch_data(data, batch_size)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(self.data, epochs=1, verbose=0, validation_data=(x_test, y_test))\n",
    "\n",
    "        for(X_train, Y_train) in self.data:\n",
    "            return self.model.get_weights(), len(X_train), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, accuracy = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        return loss, len(x_test), {\"accuracy\": accuracy}\n",
    "\n",
    "# define save strategy for showing aggregated accuracy and aggregated loss at the end of each round\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "\n",
    "    def aggregate_evaluate(self,rnd,results,failures):\n",
    "\n",
    "      if not results:\n",
    "          return None, {}\n",
    "\n",
    "      aggregated_loss, aggregated_metrics = super().aggregate_evaluate(rnd, results, failures)\n",
    "      accuracies          = [r.metrics[\"accuracy\"] * r.num_examples for _, r in results]\n",
    "      examples            = [r.num_examples for _, r in results]\n",
    "      aggregated_accuracy = sum(accuracies) / sum(examples)\n",
    "      print(f\"comm_round: {rnd} | global_accuracy: {aggregated_accuracy} | global_loss: {aggregated_loss}\")\n",
    "      accuracy.append(aggregated_accuracy)\n",
    "      loss.append(aggregated_loss)\n",
    "      return aggregated_loss, {\"accuracy\": aggregated_accuracy}\n",
    "\n",
    "# custom activation function\n",
    "def custom_activation(output):\n",
    "\n",
    "    logexpsum = backend.sum(backend.exp(output), axis=-1, keepdims=True)\n",
    "    result    = logexpsum / (logexpsum + 1.0)\n",
    "    return result\n",
    "\n",
    "# define the standalone supervised and unsupervised discriminator models\n",
    "def define_discriminator(input_shape, num_classes):\n",
    "\n",
    "     # image input\n",
    "     input_image = layers.Input(shape=input_shape)\n",
    "     # downsample\n",
    "     fe          = layers.Conv2D(128, (3,3), strides=(2,2), padding='same')(input_image)\n",
    "     fe          = layers.LeakyReLU(alpha=0.2)(fe)\n",
    "     # downsample\n",
    "     fe          = layers.Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "     fe          = layers.LeakyReLU(alpha=0.2)(fe)\n",
    "     # downsample\n",
    "     fe          = layers.Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "     fe          = layers.LeakyReLU(alpha=0.2)(fe)\n",
    "     fe          = layers.Flatten()(fe)\n",
    "     fe          = layers.Dropout(0.4)(fe)\n",
    "     # output layer nodes\n",
    "     fe          = layers.Dense(num_classes)(fe)\n",
    "     # supervised output\n",
    "     c_out_layer = layers.Activation('softmax')(fe)\n",
    "     # define and compile supervised discriminator model\n",
    "     c_model     = Model(input_image, c_out_layer)\n",
    "     c_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "     # unsupervised output\n",
    "     d_out_layer = layers.Lambda(custom_activation)(fe)\n",
    "     # define and compile unsupervised discriminator model\n",
    "     d_model     = Model(input_image, d_out_layer)\n",
    "     d_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "     return d_model, c_model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "\n",
    "     # image generator input\n",
    "     input_latent = layers.Input(shape=(latent_dim,))\n",
    "     # foundation for 7x7 image\n",
    "     gen          = layers.Dense(128 * 7 * 7)(input_latent)\n",
    "     gen          = layers.LeakyReLU(alpha=0.2)(gen)\n",
    "     gen          = layers.Reshape((7, 7, 128))(gen)\n",
    "     # upsample to 14x14\n",
    "     gen          = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "     gen          = layers.LeakyReLU(alpha=0.2)(gen)\n",
    "     # upsample to 28x28\n",
    "     gen          = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "     gen          = layers.LeakyReLU(alpha=0.2)(gen)\n",
    "     # output\n",
    "     out_layer    = layers.Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
    "     # define model\n",
    "     model        = Model(input_latent, out_layer)\n",
    "     return model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "\n",
    "     # make weights in the discriminator not trainable\n",
    "     d_model.trainable = False\n",
    "     # connect image output from generator as input to discriminator\n",
    "     gan_output        = d_model(g_model.output)\n",
    "     # define gan model as taking noise and outputting a classification\n",
    "     model             = Model(g_model.input, gan_output)\n",
    "     # compile model\n",
    "     model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "     return model\n",
    "\n",
    "# select a supervised subset of the dataset, ensures classes are balanced\n",
    "def select_supervised_samples(dataset, num_samples, num_classes):\n",
    "\n",
    "    X, y           = dataset\n",
    "    unique_labels  = np.unique(y, axis=0)\n",
    "    X_list, y_list = list(), list()\n",
    "    n_per_class    = int(num_samples / num_classes)\n",
    "\n",
    "    for i in unique_labels:\n",
    "\n",
    "        # get all images for this class\n",
    "        class_data   = [(image, label) for (image, label) in list(zip(X, y)) if (i == label).all()]\n",
    "        X_with_class, _ = zip(*class_data)\n",
    "        # choose random instances\n",
    "        ix           = np.random.randint(0, len(X_with_class), n_per_class)\n",
    "        # add to list\n",
    "        [X_list.append(X_with_class[j]) for j in ix]\n",
    "        [y_list.append(i) for j in ix]\n",
    "\n",
    "    return np.asarray(X_list), np.asarray(y_list)\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, num_samples):\n",
    "\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix             = np.random.randint(0, images.shape[0], num_samples)\n",
    "    # select images and labels\n",
    "    X, labels      = images[ix], labels[ix]\n",
    "    # generate class labels\n",
    "    y              = np.ones((num_samples, 1))\n",
    "    return [X, labels], y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, num_samples):\n",
    "\n",
    "    # generate points in the latent space\n",
    "    z_input = np.random.randn(latent_dim * num_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = z_input.reshape(num_samples, latent_dim)\n",
    "    return z_input\n",
    "\n",
    "# use the generator to generate fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, num_samples):\n",
    "\n",
    "    # generate points in latent space\n",
    "    z_input = generate_latent_points(latent_dim, num_samples)\n",
    "    # predict outputs\n",
    "    images  = generator.predict(z_input)\n",
    "    # create class labels\n",
    "    y       = np.zeros((num_samples, 1))\n",
    "    return images, y\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, c_model, latent_dim, dataset, num_samples):\n",
    "\n",
    "    # prepare fake examples\n",
    "    X, _ = generate_fake_samples(g_model, latent_dim, num_samples)\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    X    = (X + 1) / 2.0\n",
    "\n",
    "    # plot images\n",
    "    for i in range(100):\n",
    "\n",
    "        plt.subplot(10, 10, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "\n",
    "    # save plot to file\n",
    "    #filename = 'generated_plot_%04d.png' % (step+1)\n",
    "    #plt.savefig(filename)\n",
    "    plt.show()\n",
    "    # save the classifier model\n",
    "    filename = 'emnist_%04d.h5' % (step+1)\n",
    "    c_model.save(filename)\n",
    "\n",
    "    # evaluate the classifier model\n",
    "    X, y   = dataset\n",
    "    _, acc = c_model.evaluate(X, y, verbose=0)\n",
    "    print('Classifier Accuracy: %.3f%%' % (acc * 100))\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, c_model, gan_model, dataset, latent_dim, num_samples, num_classes, num_epochs, num_batch):\n",
    "\n",
    "    # select supervised dataset\n",
    "    X_sup, y_sup = select_supervised_samples(dataset, num_samples, num_classes)\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo  = int(dataset[0].shape[0] / num_batch)\n",
    "     # calculate the number of training iterations\n",
    "    n_steps      = bat_per_epo * num_epochs\n",
    "    # calculate the size of half a batch of samples\n",
    "    half_batch   = int(num_batch / 2)\n",
    "    print('num_epochs=%d, num_batch=%d, 1/2=%d, b/e=%d, steps=%d' % (num_epochs, num_batch, half_batch, bat_per_epo, n_steps))\n",
    "\n",
    "    for i in range(n_steps):\n",
    "\n",
    "         # update supervised discriminator (c)\n",
    "        [Xsup_real, ysup_real], _ = generate_real_samples([X_sup, y_sup], half_batch)\n",
    "        c_loss, c_acc             = c_model.train_on_batch(Xsup_real, ysup_real)\n",
    "        # update unsupervised discriminator (d)\n",
    "        [X_real, _], y_real       = generate_real_samples(dataset, half_batch)\n",
    "        d_loss1                   = d_model.train_on_batch(X_real, y_real)\n",
    "        X_fake, y_fake            = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        d_loss2                   = d_model.train_on_batch(X_fake, y_fake)\n",
    "        # update generator (g)\n",
    "        X_gan, y_gan              = generate_latent_points(latent_dim, num_batch), np.ones((num_batch, 1))\n",
    "        g_loss                    = gan_model.train_on_batch(X_gan, y_gan)\n",
    "        # summarize loss on this batch\n",
    "        print('>%d, c[%.3f,%.0f], d[%.3f,%.3f], g[%.3f]' % (i+1, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n",
    "\n",
    "        # evaluate the model performance every so often\n",
    "        if (i+1) % bat_per_epo == 0:\n",
    "\n",
    "            summarize_performance(i, g_model, c_model, latent_dim, dataset, num_samples)\n",
    "\n",
    "# define initial parameters\n",
    "num_classes            = 47\n",
    "# number of sub clients to be created from each non-iid class\n",
    "num_intraclass_clients = 1\n",
    "num_clients            = num_classes * num_intraclass_clients\n",
    "input_shape            = (28, 28, 1)\n",
    "batch_size             = 128\n",
    "# non-iid severity, 1 means each client will only have one class of data\n",
    "severity               = 1\n",
    "comms_round            = 10\n",
    "clientDataPercent      = 5\n",
    "# size of the latent space\n",
    "latent_dim             = 128\n",
    "num_samples            = 1000\n",
    "num_epochs             = 20\n",
    "num_batch              = 100\n",
    "accuracy               = list()\n",
    "loss                   = list()\n",
    "clientsData            = list()\n",
    "\n",
    "# read dataset from csv files and reshape it to the input_shape\n",
    "dateset_train = pd.read_csv('emnist-balanced-train.csv', header=None)\n",
    "dateset_train = dateset_train.to_numpy()\n",
    "x_train_temp  = dateset_train[:,1:785]\n",
    "\n",
    "x_train       = np.zeros(shape=(112800, 28, 28), dtype=np.uint8)\n",
    "for idx, n in enumerate(x_train_temp):\n",
    "    x_train[idx] = np.reshape(n, (28, 28))\n",
    "\n",
    "y_train       = dateset_train[:,0]\n",
    "\n",
    "dateset_test  = pd.read_csv('emnist-balanced-test.csv', header=None)\n",
    "dateset_test  = dateset_test.to_numpy()\n",
    "x_test_temp   = dateset_test[:,1:785]\n",
    "\n",
    "x_test        = np.zeros(shape=(18800, 28, 28), dtype=np.uint8)\n",
    "for idx, n in enumerate(x_test_temp):\n",
    "    x_test[idx] = np.reshape(n, (28, 28))\n",
    "\n",
    "y_test        = dateset_test[:,0]\n",
    "\n",
    "# scale dataset from [0,255] to [-1,1]\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test  = x_test.astype(\"float32\")\n",
    "x_train = (x_train - 127.5) / 127.5\n",
    "x_test  = (x_test - 127.5) / 127.5\n",
    "# expand to 3d, add channels\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test  = np.expand_dims(x_test, -1)\n",
    "# change labels to categorical format\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# create non-iid clients for simulation\n",
    "clients = non_iid_x(x_train, y_train, severity, num_intraclass_clients)\n",
    "\n",
    "# collect a percent of clients data\n",
    "for (client_name, data) in clients.items():\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "    data_temp                = data[:int(len(data) * clientDataPercent / 100)]\n",
    "    x_client, y_client       = list(zip(*data_temp))\n",
    "    x_client                 = np.asarray(x_client)\n",
    "    y_client                 = np.asarray(y_client)\n",
    "    clientgen                = ImageDataGenerator(rotation_range=5, width_shift_range=0.02, height_shift_range=0.02, shear_range=0.1, zoom_range=0.1, fill_mode='nearest')\n",
    "    clientgen.fit(x_client)\n",
    "    g                        = clientgen.flow(x_client, y_client, batch_size=len(x_client), shuffle=True)\n",
    "    x_redundant, y_redundant = next(g)\n",
    "    clientsData.extend(list(zip(x_redundant, y_redundant)))\n",
    "    np.random.shuffle(clientsData)\n",
    "\n",
    "np.random.shuffle(clientsData)\n",
    "x_clients, y_clients = list(zip(*clientsData))\n",
    "x_clients            = np.asarray(x_clients)\n",
    "y_clients            = np.asarray(y_clients)\n",
    "\n",
    "# augment the collected data from clients\n",
    "datagen              = ImageDataGenerator(rotation_range=10, width_shift_range=0.05, height_shift_range=0.05, shear_range=0.2, zoom_range=0.2, fill_mode='nearest')\n",
    "datagen.fit(x_clients)\n",
    "g                    = datagen.flow(x_clients, y_clients, batch_size=len(x_clients), shuffle=True)\n",
    "# create a new dataset based on augmented data\n",
    "x_created, y_created = next(g)\n",
    "last_step            = int(((len(x_clients) * 100) / clientDataPercent) / len(x_clients)) - 1\n",
    "current_step         = 1\n",
    "\n",
    "while current_step <= last_step:\n",
    "\n",
    "    x_temp, y_temp = next(g)\n",
    "    x_created = np.concatenate((x_created, x_temp))\n",
    "    y_created = np.concatenate((y_created, y_temp))\n",
    "    current_step += 1\n",
    "\n",
    "# create the discriminator models\n",
    "d_model, c_model = define_discriminator(input_shape, num_classes)\n",
    "# create the generator\n",
    "g_model          = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model        = define_gan(g_model, d_model)\n",
    "# train model with the created dataset\n",
    "train(g_model, d_model, c_model, gan_model, [x_created, y_created], latent_dim, num_samples, num_classes, num_epochs, num_batch)\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = c_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train Accuracy: %.3f%%' % (train_acc * 100))\n",
    "_, test_acc  = c_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Accuracy: %.3f%%' % (test_acc * 100))\n",
    "\n",
    "# start Flower simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=num_clients,\n",
    "    client_resources={\"num_cpus\": 2},\n",
    "    config=fl.server.ServerConfig(num_rounds=comms_round),\n",
    "    strategy=SaveModelStrategy()\n",
    ")\n",
    "\n",
    "# plot accuracy and loss of each round\n",
    "plt.suptitle('Accuracy Comparison')\n",
    "plt.scatter(range(1, comms_round+1), accuracy, c=[\"red\"])\n",
    "plt.show()\n",
    "plt.suptitle('Loss Comparison')\n",
    "plt.scatter(range(1, comms_round+1), loss, c=[\"red\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
